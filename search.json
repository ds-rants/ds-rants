[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS Rants Main Page",
    "section": "",
    "text": "I Will Not Attend Your Pre-Pre-Pre-Sprint-Planning Meeting\n\n\n\n\n\n\nsoftware engineering\n\n\nfake agile\n\n\nscrum clusterfuck\n\n\nmeetings\n\n\n\n\n\n\n\n\n\nApr 20, 2025\n\n\nDS Rants\n\n\n5 min\n\n\n\n\n\n\n\n\n\n\n\n\nYour Pandas Code Is Bad And You Should Feel Bad!\n\n\n\n\n\n\npandas\n\n\ncode\n\n\nperformance\n\n\nproduction\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nDS Rants\n\n\n9 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025/2025_04_20_pre-pre-sprint-planning/index.html",
    "href": "posts/2025/2025_04_20_pre-pre-sprint-planning/index.html",
    "title": "I Will Not Attend Your Pre-Pre-Pre-Sprint-Planning Meeting",
    "section": "",
    "text": "Dear Product Owner / Manager / Team Leader,\nMy heart was filled with pain and sorrow when I noticed you sent a invitation for a pre-sprint planning meeting to be held on the day before the actual event. I am deeply saddened to inform you that I will not attend any of such events, on any occasion, ever.\nHere I must stand my ground because I know what lies ahead, even if you don’t know it yourself. You will speak about the 20min sprint planning meeting being to short to write and select the tickets, you will say that there is a need for larger synchronization and cooperation within the team, you will claim that the pre-sprint-planing event will only last 10 min. You are lying to us, to the world and to yourself but you don’t know it yet.\nYou are suffering from a clear case of chronic scrumitis and sadly the only reasonable treatment is abstinence, whether it is on your own will or forced. You are trying to create more of the scourge that plagues software engineering, you have heard of it, but did not think much of it, it has crept through your own week, preventing you to do anything useful, and now you’re attempting to cure this evil by creating more of it: Meetings!\nI did not say much previously because all things considered you were pretty reasonable. Our sprint planning meetings usually lasted for their original planed 20 min, sometimes a bit more but not too much. You insisted on doing 30 min retrospectives every two weeks, and I did not complain even though it was pretty much pointless. Very little value came out of it, but you looked so happy seeing the engineers getting together and doing the “team building” activities you so proudly crafted. You even had good things for you when you tried to keep daily meetings within their planed 10 min bounds, I really command you for that feat! Sadly, you’re going down a dreaded path and you don’t know it yet.\nI have seen what will happen, you will round up a few consenting engineers who quite like you and bring them to their dooom (Fly! You fools!). They will fall for the dumb excuses you make, the nice jokes you tell, or just yield to peer pressure and fold into the line… You have already betrayed them, but you don’t know it yet.\nThat original sin that was supposed to “last 10min, yeah, I swear on my mother’s life… I also have things to do… you can trust me…”. Well now it has been 40 min and far from over. I see the despair on the faces of the same engineers I could not save from your grasp, who are now in deepest pit hell-scrum. I see their eyes drift away with a thousand miles stare as you suck the energy out of them, and soon you will ask: “How many points should we give for this story?”. You’re dangerous but you don’t know it yet.\nYou could have chosen many other paths. You could have selected carefully a few customers that you would have brought to the engineers for them to interact directly. You could have fought for these same engineers and cleared their calendar from the filth imposed by other fools, for them to start pair-programming and actually doing work. You could have let go of that little feature and its stupid story points to decide instead to write tests so that your software does not turn into a pile of crap in 6 months. You could have read a few books written by experts who faced the same issues before and found working solutions. You’re ignorant but you don’t know it yet.\nThe work could have been managed very differently. Who said that you were not supposed to take up another task during the week if you’re done early with the first? Who said that its not possible to add a new task on the board if something comes up in the middle of the work? You never said such things, yet they probably came in your mind in a malformed way. You probably imagined deep down that your engineers were not “stream-aligned”, “business-oriented”, hard-working enough, and you imagined they needed actual supervision, thus you started patronizing your team but you don’t know it yet.\nAnd soon you will find that this pre-sprint-planning meeting does not produce the results you hoped for. Your team does not deliver enough features, you still think they need better synchronization, more teamwork and stronger engagement. So you will think really hard about it because you’re not dumb, because you have faced issues before and things kinda worked out, we just need more of all those things. You will work, fight, overcome this hardship and you will do it:\nYou will add another meeting to prepare for the pre-sprint-planning meeting.\nNow, you know.\nSincerely"
  },
  {
    "objectID": "posts/2025/2025_04_your_pandas_code_is_bad/index.html",
    "href": "posts/2025/2025_04_your_pandas_code_is_bad/index.html",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "",
    "text": "That’s it! Three times! The third time is the charm as they say!\nIt has been three times, THIS VERY WEEK, that I’ve been pulled into a meeting because someone had a performance problem with their pandas code. Each time it’s the same problem: some kind of genius decides to remove all safety belts, starts hacking and typing until their code regurgitates something. Then that person just hopefully and magically wishes that it will produce the expected outcome. Well, the outcome is me being called, and I’m deeply aggravated:\nIF YOU HAVE FOR LOOPS IN YOUR PANDAS CODE, YOU ARE WRONG!\nThere, I said it…"
  },
  {
    "objectID": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#the-not-so-good",
    "href": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#the-not-so-good",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "The not so good",
    "text": "The not so good\nThe first time was a young data analyst who produced a notebook to perform some anomaly detection on curves. It was running slow even though it was running on a GCP machine that could probably crack half of the Internet passwords in an hour. The first rookie mistake was trying to plot about 500 examples of curves with about 5000 points per curve because the sampling rate is under a millisecond. I mean, plotly is a great library, no doubt about it, but that is a lot of things to print on your screen. The machine breathes a sigh of relief…\nBut the signs of abuse were deeper. I will overlook the code blocks with a width on screen of 700 characters because everyone loves to scroll right and left (Please just run ruff on your notebooks). I will purposely ignore the function that does data pre-processing AND plotting, for now… But I cannot turn a blind eye to the double-nested for loop with print statements every 10ms and in-place modification of your gigantic dataframe that propels your machine into locked-in syndrome.\nAlright, nothing that can’t be fixed with a little bit of explanation and starting from the basics: down-sampling large data, method chaining in pandas, use vectorization it is your friend, and no for loops, please."
  },
  {
    "objectID": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#the-bad",
    "href": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#the-bad",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "The bad",
    "text": "The bad\nThe second time felt awfully similar. I got called for support by a senior contractor because their code on our MLOPS pipeline in Vertex AI had crashed, apparently due to an out-of-memory error. For those who don’t know, Vertex AI is a GCP service that allows you to run machine learning jobs, using Kubernetes under the hood, so basically you can scale up the machine running your job quite a bit. Any data scientist would think logically that the thing crashed during the training, but it was the prediction part that had crashed, AFTER RUNNING FOR TWO DAYS!\nWhat kind of unholy job are you guys running there? Ah, you’re running a PyTorch model to classify texts, great! Are you trying to classify the whole internet for it to take this long? What’s the size of your dataset? 20K text samples and the whole size is around a few 100s MB? Mother… I don’t know how you’re doing it, but you really managed to mess this one up. In the absolute worst case, this should run in a matter of hours, not days!\nAlright! Time to deep dive and see what ungodly horrors lie at the bottom of this…\nMy good sir, I thank you for creating functions, but yours are 50+ lines long, take 10 different arguments, and modify things in place… I hope you have tests for this (who am I kidding) … Anyway, where is your logic for the prediction? … Within a class with snake-case naming, not the worst thing I will see today, I am sure… Ah, this is the meat of all, the “predict” method… And…\nNO! GOD! NO! PLEASE! NO!\nYou sir, are guilty of the sin of abusing the for loop and pd.concat pattern. Don’t tell me you are redoing this for each line of your dataframe? Of course you are! Are you even aware that for each line you are basically writing a whole new dataframe, which according to you is several 100s MB, that contains just an extra prediction compared to the old one, then doing an in-place operation to replace the old one, and you are wondering why your code takes this long? You’re not even wondering because it did not occur to you that such a job running for several days was a horrible mistake. Oh, and by the way, do you know that you have a triple-nested for loop here in this sub-function of your method “predict”?\nYeah, this thing is going places."
  },
  {
    "objectID": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#there-is-nothing-sacred-anymore",
    "href": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#there-is-nothing-sacred-anymore",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "There is nothing sacred anymore",
    "text": "There is nothing sacred anymore\nThe pinnacle of incompetence was definitely reached by a few members of a consulting firm, you know one of the BIG players that sell their meat “highly trained experts” for a modest 2000-4000 dollars a day. Same scenario, I get added to a meeting whose objective was to determine if the MLOPS pipeline we developed can accommodate scaling up the model they developed because currently their prediction is taking 3-4 hours and they have only implemented 1 of the 100 possible cases. Obviously, it will be very difficult to run the job for all cases within a day. Obviously, they are using the largest machine they can for their Vertex AI job. Obviously, they are trying to say that their “high-end” solution is in danger, although we paid a hefty price for it. Obviously, it’s about blame shifting. Sadly, for them, I already know that they are full of shit.\nThe first reason is that a few days ago, I had already heard of one of their “great” engineers reaching for help because his highly efficient unit tests were not working in the CI because he did a bunch of calls to Google Storage and BigQuery in them. Calling the database in unit tests, genius… But the second and most important reason why they are full of shit is my secret weapon. It is because our generic MLOPS pipeline does quite a bit between jobs but almost nothing within the job per se, just a little bit of I/O at the start and the end, everything else is just the logic implemented by the “customer” data scientist. In other words, if things go sideways during the job, then you sir, you are the problem! Sorry, I’m not sorry.\nLet’s speed up the next 25 minutes of the discussion:\nSo, let’s see the logs… Apparently, so you have about 100 configurations and for every one of them you’re spending many minutes in method X doing some preprocessing… let’s pull up the corresponding code… And…\nNO! GOD! NO! PLEASE! NO!\n\n1default_dict_of_df : dict[str, list[pd.DataFrame]] = ...\n\ndata_per_provider = {\"total\": [], **default_dict_of_df}\n\nfor provider in dataset[\"provider\"].unique():\n    provider_df = dataset[dataset[\"provider\"] == provider]\n\n    data_per_provider[\"total\"].append(\n2        generate_feature_time_series(\n            provider_df, date_index, provider\n        )\n    )\n    for feature in list_of_feature:\n3        for value in provider_df[feature].unique():\n            data_per_provider[feature].append(\n4                generate_feature_time_series(\n5                    provider_df[provider_df[feature] == value],\n                    date_index,\n                    provider,\n                    feature=feature,\n                    value=value,\n                )\n            )\nprovider_stocks_dfs = {\n    feature: pd.concat(data_per_provider[feature], axis=0)\n    for feature in [\"total\", *list_of_feature]\n}\n\n1\n\nJust the type of this thing sounds already like a bad idea\n\n2\n\nTwo pivot and two reindex operations in there. Bye bye performance…\n\n3\n\nTriple for loop nesting, aka. the speeding ticket of computer!\n\n4\n\nAgain, two pivot and two reindex operations in there. Bye bye performance…\n\n5\n\nNotice calling the same function with a different granularity, pure art!\n\n\nYeah, triple-nested for loops, combined with apply-pivot-append operations with concatenation in the end, if this was figure skating you would probably score lots of points. Too bad you are trying to do data science.\nIn the end, a team of so-called “expert” consulting engineers wrote something so egregious it threatened the whole project. Any junior writing such a thing should be taught better. None of them bothered to check the logs to investigate the issue. Oh, and by the way, the huge machine they were using was running at 20% of CPU and RAM load all the time, they did not check that either! Oh and they did the same mistake in two places within the codebase, so now we have to wait twice as much.\nI guess shipping garbage that barely works is definitely a good way to get your contract renewed and be paid to continue maintaining it… But this is a different story."
  },
  {
    "objectID": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#final-thought",
    "href": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#final-thought",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "Final thought:",
    "text": "Final thought:\nSo please kids, do not abuse your pandas. Keep them safe from the bad influence of for loops. Believe me, it’s much easier to make your code not suck rather than spending days trying to optimize a clunky piece. I fully agree here with what Casey Muratori says about non-pessimization and this needs to be heard everywhere:\n\nNon-pessimization, it’s simply not introducing tons of extra work for the computer to do!"
  },
  {
    "objectID": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#disclaimer",
    "href": "posts/2025/2025_04_your_pandas_code_is_bad/index.html#disclaimer",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "Disclaimer:",
    "text": "Disclaimer:\nThe style of this post may remind some of you of the comrade Ludicity and I must confess I am a deep fan and an active reader of his work.\nThough I tried not to imitate his style too much, this post MUST be a strong rant because some things need to be said out loud. Also, because of my own swearing nature and my shortcomings in writing, I doubt that I managed efficiently to do so. Ludicity, if you’re reading this, I apologize deeply."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  }
]