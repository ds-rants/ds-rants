[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I like data science but I hate unnecessary complexity, convoluted things and a despise projects made up to answer problems that are not even there.\nThis site will provide a collection of reflections on hopefully modern data science problems, the bloat surrounding them, and the sad fact that most of projects completely fail or just don’t bring any value whatsoever.\nSwearing will be abundant, people doing stupid things will be called stupid and more…\nThankfully, there is no amount of swearing that cannot be nicely illustrated by memes, so buckle up!\nAnd Enjoy!\nPS: Don’t hesitate if you want to complain like I do (data.science.rants@proton.com)."
  },
  {
    "objectID": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#big-brain-moment",
    "href": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#big-brain-moment",
    "title": "Y’All Motherfuckers Need Tests",
    "section": "Big Brain Moment",
    "text": "Big Brain Moment\n\n“To me, legacy code is simply code without tests.”\nMichael C. Feathers, Working Effectively with Legacy Code\n\n\n“With tests, we can change the behavior of our code quickly and verifiably. Without them, we really don’t know if our code is getting better or worse.”\nMichael C. Feathers, Working Effectively with Legacy Code\n\nNote: Big Brain Reference"
  },
  {
    "objectID": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#i-am-a-simple-data-scientist-are-you-sure-i-need-tests",
    "href": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#i-am-a-simple-data-scientist-are-you-sure-i-need-tests",
    "title": "Y’All Motherfuckers Need Tests",
    "section": "I Am A Simple Data Scientist, Are You Sure I Need Tests?",
    "text": "I Am A Simple Data Scientist, Are You Sure I Need Tests?\nWhich blog are you on? Data Science Rants! Of course you need tests!\n\nWhy Should I Test?\nBecause it would be nice to have a data science project working once in a while that isn’t a complete dumpster fire. Just for context, for developers nowadays the question isn’t “should I automate my tests?”, but rather “how, when, and on which infrastructure to mirror prod as closely as possible, and independently from other people working on different features?” All senior developers advocate for regular and comprehensive testing strategies. The “State of DevOps” reports show testing as a major component of team performance and software reliability.\nSure, a lot of ‘recent’ advances in the developer and DevOps world have yet to make their entrée into the data world (Data: The Land That DevOps Forgot). One could say we’re poorly equipped in terms of tooling, but one could also say we missed the fucking memo. Some alleged data scientists will happily tell you they don’t version their code without trembling… Also, good luck convincing a junior DS to do anything outside their notebook without throwing a panic fit, or to fully automate their model selection/deployment via a simple CI/CD pipeline. You’ll get an idea of what I mean.\nYet, data science folks are still curled up in a fetal, near-autistic position regarding developer best practices, meaning these concepts are usually totally absent from our average workflows. In the end, why should we adopt something so different from our ways?\n\n\nBecause Most Data Science Projects Never Reach Production\nThis means we’re wasting time, money, and worse, effort in dead projects when we could be doing more interesting and useful things for society. But it’s fine because we’ve always been told companies are the pinnacle of efficiency.\nHowever, this bitter state of the data science profession is grounded in some harsh realities. In addition to the absence of software engineering best practices, we’re usually in a much worse situation than typical developers. Indeed, we inherently have a particularly strong coupling to the data (shocker, right?). Even small changes over time in data distribution tend to have a tremendous impact on machine learning systems, even though the data’s structure itself stays the same. And you know when that happens? All the fucking time, for every damn use-case!\nFinally, we usually need to produce large amounts of code just to determine if a given model or analysis has even a chance of being remotely useful. This creates a tension because the code seems constantly in a superposition of states: useless exploratory junk AND awesome preprocessing for a big-gains model. Then some very natural cognitive mechanisms kick in: “I don’t need a test because I don’t know if my code has any worth”. It’s too late at this point because we’ll see the tremendous forces that prevent us from writing tests after the code.\nFacing these difficulties requires discipline, and one hell of a kind! We need to import best practices from other IT disciplines that have demonstrated effectiveness in creating reliable software.\nOne of these practices is testing, so let’s briefly recap some generalities.\n\n\nThe Main Types of Tests\nUnless you’re living under a rock, you’ve probably heard of unit, integration, and acceptance testing. For a good definition, check out Martin Fowler’s Website. In the meantime, my pseudo-standard definitions are:\n\nUnit tests: Performed to validate the behavior of isolated functions and methods (more rarely classes and modules). Fast: &lt; 1 ms. My personal take: No dependency on I/O or external systems (disk access, internet, database…), with absolute control of inputs and precise evaluation of outputs.\nIntegration tests: Performed on a collection of functions, usually at the class and module level. Slower in general, but a few seconds max. I/O becomes possible, but my hot take is: you should only interact with local elements (mostly disk, local database replicas, components, or binaries from other subsystems) and nothing that touches the internet.\nAcceptance tests: Performed on critical system paths to ensure key functionalities. Much slower, aiming for around a few minutes or even less. Otherwise, they’re run less often and start losing value. Here, you can interact with other external systems over the network, databases, and such, but beware, beware!\n\nThese three types of tests constitute the cornerstone of a good, fully automated test suite that will allow you to determine with confidence if your system is working and behaving as expected. Now, one question remains: when should you write your tests?"
  },
  {
    "objectID": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#adopting-a-testing-strategy",
    "href": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#adopting-a-testing-strategy",
    "title": "Y’All Motherfuckers Need Tests",
    "section": "Adopting A Testing Strategy",
    "text": "Adopting A Testing Strategy\n\nNo Tests At All\nYou, sir, are dangerous. You should be thrown in jail, and your ugly, dishonest, and deceiving notebook code should be lit on fire. You’re basically handing over a pile of garbage to your coworkers and yelling as you exit the building:\n\n“And good luck with any changes in business requirements, package version upgrades, refactoring, or general future evolutions, because you know it works on my machine… Oh, and by the way, the business wants this model in production next week…”\n\nI despise you with every fiber of my being and will definitely high-five you at the first occasion… in the face… with a shovel!\n\n\nWriting Tests After Writing The Code\nEither you have good intentions but don’t know how to do it, and your heart is in the right place. Or you’ve been told to write tests by a concerned tech lead because prod was on fire three times this week, and you reluctantly follow orders like a good soldier. Regardless, there are a few problems with that approach:\n\nAssuming you’re not a complete moron, you have at least run and tested your code manually, visually, or made sure it compiled. Strangely, the idea of writing a test already starts to lose meaning because you just saw it with your own eyes: the code is running. This mental block is, to me, by far the largest factor that prevents juniors from realizing the value of automated tests.\n\n\n\n\n\n\nImportant\n\n\n\nReally, that’s the main danger. The code runs! You just saw it. Then the brain automatically disconnects, and the lingering thought comes up: “Do I really need a test?” This is where it all ends.\n\n\nIn addition, writing the test afterwards will be extremely painful and difficult because your code has not been written with testability as a core requirement. Then you’ll need some decent luck and great efforts to isolate deterministic behavior in 100 lines of spaghetti with mutations everywhere and no clear responsibilities.\nAs a direct consequence, tests also tend to become coupled with implementation details, making them brittle or flaky, and generally more difficult to maintain. The typical example is when you want to change one line in your production code; you clearly see the change, but then you also have to change 20 tests… This is usually a sign of bad coupling between tests and implementation.\nSome large chunks of the system will very likely escape any form of testing (a consequence of reason #2) because of the impossibility to control their inputs and outputs.\n\nRegardless, writing tests after will probably be a lot of pain, especially if a higher-up dropped a decaying legacy project on your lap. In which case, you should turn to experts who will tell you how to effectively test and manage the complexity of legacy codebases (remember: code without tests).\n\n\nWriting Tests And Code At The Same Time\nNow we’re finally getting somewhere. Most of the issues mentioned in the previous section start to erode, with the main exception of reason #3. The main risk with writing tests concurrently with code is increasing the coupling between the test and the code beyond what is strictly required. This reduces maintainability and limits future changes and evolution.\nHowever, there are cases where this approach can be fruitful, especially for certain scopes and contexts (more on that later).\n\n\nWriting Tests Before Any Code\nAt last, for any sleeping data scientist who managed to open an eyelid, this is the bread and butter of any self-respecting developer these days. This is called Test-Driven Development, a.k.a. TDD, and this is how to do it properly:\n\nRED: You write a failing test, which, from well-defined inputs, asserts that a given piece of code has certain well-defined outputs. This is akin to writing specifications in the code.\nGREEN: You write the smallest, most simplistic, even idiotic code you can think of to make the test pass. Nothing more!\nREFACTOR: You replace the idiotic part you just wrote with something less stupid or ugly. But pay attention! You should not write anything that is not inside the test. The point is to let the tests and specifications drive the code.\n\nAnd you repeat the cycle!\n\n\nThe Advantages Of TDD\nThis (apparently contrived) way of testing has dramatic advantages:\n\nIt completely breaks the idea of: “I don’t need a test because I just saw my code run” because the code doesn’t even exist yet.\nYou are writing specifications for the observable behavior in the tests, not implementation details, meaning tests and code are less coupled.\nIt forces you to write code in very small, incremental steps, and all the great old wizards of software engineering are pretty much saying this is the best way to work, period!\nYou are usually able to fall back to working code with a bunch of automated tests within a few seconds. This is such a change that Kent Beck, the guy who coined the term TDD, called it Xanax for developers!\nThe code becomes easier to test because you’re writing with a clear goal to make it testable; otherwise, you’re just making your life difficult. Duh!\nThe incremental nature forces a progressive decoupling between the code and the test, which is a great sign for maintainability and increases the possibility of future changes.\nBecause you want things to be easy to test, you naturally limit the scope of what your functions/classes can do, which increases their internal cohesion. They tend to do fewer things but do them better.\n\nOverall, writing tests first has such a strong impact on your design. Namely, it prevents you from doing stupid shit, like becoming your worst enemy and face-planting your project 3 meters underground. You will actually keep the possibility to continue making changes to your code (see the excellent YouTube channel Modern Software Engineering). Ain’t that good enough in itself?\n\n\n“But Sir Rants, if TDD is that good, why are data scientists not doing it?”\n“Young lad, thank you for the very interesting question, allow me to introduce you to: …”"
  },
  {
    "objectID": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#the-top-7-obstacles-that-prevent-you-from-testing",
    "href": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#the-top-7-obstacles-that-prevent-you-from-testing",
    "title": "Y’All Motherfuckers Need Tests",
    "section": "The Top 7 Obstacles That Prevent You From Testing",
    "text": "The Top 7 Obstacles That Prevent You From Testing\nDear reader, I apologize for the sudden surge of click-bait writing style, but this will likely be a long post, anyhow… Actually, I am not sorry at all. I am simply taking into account your Ritalin-infused brain equipped with the attention span of a cocaine-high chihuahua.\nWriting tests is difficult, even for seasoned developers. However, writing tests first with TDD is actually easier but also requires a strong shift of mindset. In this bit, we’re going to focus mostly on unit tests. Reminder: the features of good unit tests are: fast, deterministic, reproducible, decoupled from implementation details.\n\nMost people advocating for the practice of TDD will repeat that it’s an acquired skill, one that you need to train regularly. It will take you time and practice to master it. You can obviously set yourself up for failure by jumping directly into an old legacy project when you’ve never written an automated test. That sounds like a totally reasonable thing to do.\n\n\n\n\n\n\nTipSolution\n\n\n\nStart practicing regularly with simple code katas. Do not attempt to test legacy codebases or data science workflows yet. It will very likely take you at least 3-4 months to get comfortable with the basics of testing.\nIf possible, use a small side project at work where your hands are free to move, safe from the nasty clutches of a greedy product manager.\n\n\nThe things you’ll try to test at first will probably be too large, and the scope poorly defined. You can try to test the thing that has 10 input parameters and 3 different possible output types. Sure, you can try…\n\n\n\n\n\n\nTipSolution\n\n\n\nTry to really limit the size and scope of functionalities you try to test at a given time. Take a smaller approach. When you think this is small enough, make it even smaller!\n\n\nOne very common error is to try to test the main functionality of a future piece of code right from the get-go. Rookie mistake! This is like head-butting a piece of concrete to make a wall fall down; it might work, but you might not be able to repeat that feat once your skull is cracked open.\n\n\n\n\n\n\nTipSolution\n\n\n\nDo not jump directly to the core of the functionalities; start with the simple things and let the behavior progressively emerge from the iterative process. Let’s take an idiotic example: imagine you want to sort elements of a list. Rather than giving a list of 10 numbers and making sure they are sorted in the end, start by passing an empty list (Great! We just found an edge case!), then perhaps a list with one element (shocker, input and output should be the same…).\n\n\nOnce you get the basics, please be sure to wait until you discover some catastrophic production failure before incorporating any kind of testing strategy. You definitely want to lose financial transactions first or have a very strong bias in your model exposed to your users. This should motivate you. Now let’s back up a bit, and imagine after 6 months of crunching and regurgitating code, you’re saying to your Product Manager that suddenly you want to write tests because one day you woke up and decided you cared about quality (hopefully after reading this rant).\nNow imagine that at the start of your project, you said to that blissfully unaware PM: “There shall be a test for every bit of code produced, and no test means no code, means no feature!” The only possible answer for that PM will be: “Amen.”\n\n\n\n\n\n\nTipSolution\n\n\n\nStart adopting a testing strategy as soon as possible in the project’s lifetime, and hold onto it like your grandma holds onto the steering wheel while plowing through a crowd. In the extremely likely case of a legacy project, you must be aware that some bits will be impossible to test. This is where having practiced in isolation first (step 1.) will save you on more than a couple of occasions:\n\nFind the separations inside the codebase where testing is possible in isolation; move it apart from the rest of the junk.\nTest the newly produced code.\n\n\n\nPlease make sure to make your tests as slow, flaky, and brittle as possible by systematically reading from the disk or, worse, the network or the database. This way, you’ll be sure the tests are never run because they take 3 hours to complete, and you never know why they fail.\n\n\n\n\n\n\nTipSolution\n\n\n\nStay away from any kind of I/O, i.e., disk reads or writes, network calls, database calls, cloud interaction. You should refrain from committing such ungodly horrors. They do not belong in your unit-testing strategy, and even in integration testing, use them only if they REALLY CANNOT BE AVOIDED.\n\n\nNot knowing your testing framework is a pretty good way to butcher and obscure your test suite. Similarly, you can abuse mocking and stubbing your classes and functions to ensure that nobody, not even you in two weeks, understands whatever the test is doing. Continue in that direction, and you will even create tests that assert nothing at all!\n\n\n\n\n\n\nTipSolution\n\n\n\nTake the time to understand how your testing framework is designed; what it allows you to reuse and what you should rewrite. Limit the mocking to the strict minimum to avoid dependencies on external services, but beware of their dark side.\n\n\nNot properly setting up your local environment and not learning to use your IDE/text editor is a very good way to set yourself up for failure. I saw you clearly wallowing in self-inflicted pain because you manually renamed each of the 10 occurrences of your variable when I forced you to get rid of the thing you so proudly named “temp”. Your hesitation was palpable when I spoke about extracting a method, and you reached for the copy/paste like a headless chicken. How can you hope to write tests if you can’t even navigate your codebase and perform the most basic actions to change it?\n\n\n\n\n\n\nTipSolution\n\n\n\nAlthough it may appear secondary, you absolutely need to properly configure and be comfortable with your local environment. Otherwise, it will become a mental blocker and a hidden obstacle to your workflow. Install test extensions or plugins for your language inside your editor. You need first-class support to be able to run a single test, or all of them for a given class/file. Your whole test suite should run at will, using a single click or a small terminal call.\n\n\n\nThis list of recommendations is a watered-down version of the many pieces of advice available in the sources and links of this rant. Please go check them out. Or don’t; I’m not the boss of you, but then don’t complain if you have a ludicrous 3-week ping-pong workflow with the Devs and Ops teams to put your scrappy model in production…"
  },
  {
    "objectID": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#wrapping-up",
    "href": "posts/2025/2025_11_yall_motherfuckers_need_tests/index.html#wrapping-up",
    "title": "Y’All Motherfuckers Need Tests",
    "section": "Wrapping Up",
    "text": "Wrapping Up\nThe purpose of this rant is not to help the greedy manager piling up more tasks on the shoulders of burned-out developers, nor to give them excuses to say: “You need to be more productive”. My goal is to give you tools and feedback that have been shown to reduce mental workload and make projects more manageable. I want you to be able to free up some time to reflect on your own data science practices.\nI don’t necessarily want you to use TDD everywhere. But it is one of the fundamentals of software engineering, and like any basic move, one should master it before claiming they don’t need it.\nMastering the fundamentals of testing is probably the best chance we have to integrate quality into data science. This much-needed desire for quality will allow us to kill dangerous or useless projects from the start and select the ones that should live. We are definitely going to require that if we want to do anything remotely useful in our society.\nAnd for that, we will see next time the specifics of testing in data science, but in the meantime, let’s keep in mind:\n\nPrimum Non Nocere\n\nFirst, do no harm.\n\nGood Reading\n\nModern Software Engineering - David Farley - ISBN: 9780137314867\nExtreme Programming Explained: Embrace Change - Beck, Kent; Andres, Cynthia - ISBN 10: 0321278658 / ISBN 13: 9780321278654\nTeam Topologies - Matthew Skelton; Manuel Pais - ISBN: 9781966280002\nAccelerate: The Science of Lean Software and DevOps - Gene Kim, Jez Humble, Nicole Forsgren - ISBN 10: 1942788339 / ISBN 13: 9781942788331\nTest Driven Development: By Example - Kent Beck - ISBN: 9780321146533"
  },
  {
    "objectID": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html",
    "href": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html",
    "title": "Y’all Motherfuckers Need Tests, So Stop Making Lazy Excuses",
    "section": "",
    "text": "This is the second part of a larger rant on tests with the first part available here.\nIn the first part, I mainly addressed the canonical definitions of testing, the typical difficulties that arise, and potential strategies to progressively get better at testing. By now, with the bold assumption that you possess a shred of common sense (and survival instinct), you should at least feel strongly that testing in data science should have much greater importance, to say the least.\nBut sadly, there’ll always be some junkie data scientists snorting LinkedIn posts and drunk on Jeff Bezos’ apocryphal success stories, who will tell you how special their work is, how testing doesn’t work for them, and how it would only slow them down. Despite my strong desire to caress the frontal lobe of such a person with a shovel, vaporize them on the spot, and curse all their family branches for the next 13 generations, I’ll momentarily refrain from violence.\nInstead, I’ll try to address some reasonable objections one could raise regarding the quirks and specificities that make testing in the data world trickier. But don’t mistake this moment of kindness for any form of weakness; at the end, you’ll have no excuses left."
  },
  {
    "objectID": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html#the-specific-problems-with-testing-in-data-science",
    "href": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html#the-specific-problems-with-testing-in-data-science",
    "title": "Y’all Motherfuckers Need Tests, So Stop Making Lazy Excuses",
    "section": "The Specific Problems With Testing In Data Science",
    "text": "The Specific Problems With Testing In Data Science\nAs mentioned previously for those sleeping in the back of the classroom, data scientists largely deal with large amounts of… data. This means the first action is typically to pull something from a database (lucky you), read a decently structured CSV (we all have to eat), a manually filled Excel file (fly, you fools!), or something far worse—and believe me, you don’t want to know. Do you remember what I said about testing—let alone TDD—being difficult with I/O and external coupling?\n\nThe Hidden Forms Of Couplings\nIn addition, the testing process is particularly difficult because most of the objects we deal with are collections of items, usually quite complex. Now add a sprinkle of types and object structures which are usually looser than your own mother’s morals. On top of that, these items are usually independent only in the best-case scenarios but most likely related to each other in some nasty way. How many times have you had to do an analysis in which one record at a given time strongly influenced what was to be done on surrounding elements, but only until variable B changed to a different value?\nYou don’t believe me? Let’s assume, for ungodly reasons, that you can only use 50% of your dataset for training your model: are you keeping the latest 50%, the oldest, or randomly discarding data? Sure, we can set aside this scenario, which is as plausible as Mark Zuckerberg giving a rat’s ass about your privacy or the people he fired. But I cannot recall, since I started working in the industry, a single real-life dataset in which time played no role at all in some weird way or with disguised influence (say goodbye to your Kaggle/tutorial static datasets).\nThese questions can remain somewhat secondary in the pure ‘developer’ world. Indeed, as long as the objects themselves remain valid and the code processing them remains correct, the system works properly. This is a strong yet hidden form of coupling that is difficult to circumvent in automated tests but is a major concern for any data science workflow.\nAnd it has only just begun…\n\n\nIssues With Dataframe-Like Testing And Remediation Strategies\nFor a data scientist, writing meaningful unit tests may require a decent amount of boilerplate even for simple data and light transformations. This assumes processing done largely on some kind of dataframe/array (which will encompass 95% of pandas and numpy junkies).\nI will assume you have a shred of decency for your coworkers, meaning your transformations look like this:\ndaily_awesome_calculations = (\n    original_record_with_meaningful_name\n    .drop_nulls()\n    .filter(...)\n    .group_by(\"day\", \"categorical_variable_B\")\n    .agg(...)\n    .sort()\n)\nIf not, please go see here and there. In the absence of any improvement on this matter, I will eagerly greet you with a shovel, as mentioned earlier. You have been warned…\nHere, with the use of DSL (Domain-specific Language) like pandas/polars/numpy, it can be tricky to determine if you are actually testing your custom logic or retesting the methods of a library that are already (hopefully) battle-tested.\nFor a dataframe test that is not excruciating to write, one needs to strip away all surrounding complexity. This requires the ability to use the smallest number of rows and columns. A good rule of thumb is usually 2-Rows * x-Columns or 3-Rows * x-Columns to get you started. If you can manage with only one row, you should!\nRegardless, here are some general pointers:\n\n\nSimple Advice For Writing DataFrame Tests\n\nSeparate DataFrame and pure Python logic as much as you can. Generally speaking, mixing pandas and Python is a sure-fire way to make your code slow and, in most cases, false. Additionally, Python logic is much easier to perform unit tests on.\nUnit testing on DataFrames is still possible with very small examples. Keep things as simple as you can with very small data subsets, here again, shapes of 2-Rows * x-Columns or 3-Rows * x-Columns or even smaller are your best friends. Your favorite LLM overlord can help you with the first draft of the boilerplate. Once you have the general structure, it can usually be reused on multiple tests.\nMake the data inside the example dataframe as plain, idiotic and stupid simple as possible. You should aim for the average cryptobro level here, especially during test setup or ‘given’ stage. This will likely decrease the coupling with your implementation code. It will also increase the maintainability of your test in the long run.\ngiven = pd.DataFrame(\n     {\n         \"station\": [\"a\", \"b\"],\n         \"temperature_in_celsius\": [1.1, 2.3],\n     }\n )\nis better than:\ngiven = pd.DataFrame(\n     {\n         \"station\": [\"NZ_EXT_1\", \"DE_INT_42\"],\n         \"temperature_in_celsius\": [1.132554, 2.3738687],\n     }\n )\nTesting on DataFrames requires more skill and time to get used to than non-data science code. Again, practice testing on pure Python first before jumping straight into the ocean of complexity. Similarly, practice on things with a smaller scope at first.\nBe wary of the scope: with too many steps and too much logic, you might have to make your tests extremely permissive. For example, just checking that you have more rows in your DataFrame after 30 transformations is unlikely to be highly informative…\n\nAgain, combining those ideas requires a fairly decent bit of expertise to determine the seams/separations in your code to make testing easier."
  },
  {
    "objectID": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html#testing-at-larger-scale",
    "href": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html#testing-at-larger-scale",
    "title": "Y’all Motherfuckers Need Tests, So Stop Making Lazy Excuses",
    "section": "Testing At Larger Scale",
    "text": "Testing At Larger Scale\nThere is a sort of opposition between scale and the ease of applying TDD principles. Indeed, the cornerstone of a good testing strategy is to obtain a fast, reliable, and deterministic feedback that allows the developer to have confidence that the proposed changes are safe to release. Accounting for all use cases in the integration and acceptance steps would make the testing unnecessarily slow, bulky, and probably as useful as a statement from Sam Altman on AGI.\n\nWhat About Integration Testing?\nThe role of integration testing is to make sure that the sub-components and modules of your system work properly and fit together to produce some expected behavior.\nIn the Data Land, integration tests can usually be done by taking a larger chunk of your datasets (plural, because have you ever seen a project with just one…). For example, in Machine Learning workflows, you can try to pass the smallest possible amount of data through your preprocessing, then assert that you indeed got rid of certain columns and managed to remove nulls in certain others.\nYou can then continue passing this small sample through your training and at least make sure that the training can start. There is a tremendous difference between fully automating this and using a pseudo-manual verification (either inside a notebook or via a CLI that you might forget to type).\nAgain, the point here is to ensure that preprocessing, training, evaluation, and similar components can feed into each other. We are not talking about model performance or hyper-parameter tuning.\n\n\nAcceptance Testing In The Data World a.k.a Evaluation?\nTraditionally, acceptance tests are designed to verify the bare-minimum working state of your system, again, by validating the critical or major hot paths within your application. Similarly, they also tend to appear later in the life of IT projects, once the scope becomes clearer.\nWith them, you should be able to obtain a definitive answer to the question: “Is my system in a working state so that it can be deployed?” In that regard, data science systems make no exception. This is where you’ll be able to plug in your database, buckets, and APIs.\nIn ML workflows, the automated evaluation of a model’s performance and its comparison with a previously deployed model can usually be a good approach for acceptance testing. However, the scope of this question is so large that tons of books have been written on the subject, and sadly, this blog has very little to offer in comparison (especially on the politically correct side of things…). The final strategy will vary greatly depending on the size of your data, the necessary retraining frequency, the selected metrics, and the type of model considered. The main point remains: automate this as much as you can!\n\n\nThe Holy Grail Of Data Tests\nData tests, also called defensive testing, are here to validate… the data! They stand in a weird position because we mainly see them at the level of the whole data pipeline, although they can fit at many levels of the testing pyramid. These tests ensure that you actually have a primary key (unique and non-null for the sleepy ones), that numeric columns fit within a certain range, and that you got rid of nulls…\nThis is where dbt, dataform, SQLMesh, and similar SQL-based frameworks really shine by allowing you to test data at a large scale. Using such frameworks, it is still possible to write code using TDD. Simply put, rather than starting to write SQL code, you will start by specifying in the metadata the types and tests for a given column. They are obviously SQL-oriented, but their contribution to our field is really a game changer.\nIn Python, few packages have been developed to help you use a TDD workflow in data science. One issue is perhaps that they tend to be quite different from the DSL you are working with (while Kent Beck says that a testing framework should be in the same language as the code for TDD). Another problem is that none of them have managed to obtain the same reach and influence as the SQL-based ones, with the notable exceptions of great-expectations (which can be a little quirky and slow to use) and the great pydantic (more oriented for APIs). The catch, again, is that they are not necessarily fitted for TDD in Data Science.\nOther defensive analysis packages for pandas, such as engarde or bulwark, receive less maintenance than your great-great-great-grandparent’s tomb. They also have a MAJOR design flaw, because someone thought it would be a good idea to promote a decorator-oriented approach. Clearly, simple designs are for weak-minded people. Similarly, type hints, which have been around since 2015, have been carefully omitted to ensure that after one step my IDE has no fucking idea of the object type it is currently dealing with. That is, again, not ideal for TDD.\nOn the polars side, there is one obscure package called pelage, providing fairly similar functionality to dbt tests. Thanks to a total absence of marketing, the level of adoption is currently on par with the number of working brain cells of the average Elon Musk fanboy…\nIn the end, if you have the possibility to jam as many tests as you can within your SQL pipelines, please, knock yourself out! Worst-case scenario, your analytics tables might end up with an actual working primary key, which would be a nice change for once… just saying… And who knows, you could accidentally end up with a dashboard in which the numbers are not just straight-up lies or a rough estimate, but are actually accurate…"
  },
  {
    "objectID": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html#conclusion",
    "href": "posts/2025/2025_11_yall_tests_for_ds/test_for_ds.html#conclusion",
    "title": "Y’all Motherfuckers Need Tests, So Stop Making Lazy Excuses",
    "section": "Conclusion",
    "text": "Conclusion\nThis glorious landscape—made of lying dashboards, misleading analyses, and an astonishing 80-90% of machine learning projects that die without reaching production—is just a clear sign that our profession still lacks discipline. We need to incorporate better practices into our work. It may take many different forms, but I am sure that testing will and should be a part of it.\nCertainly, there are specific issues with data science that make it more difficult to test. Granted, you might not know if you are going to keep that freshly made 500-line analysis. But let’s be real for a minute: the fact that your 500 lines haven’t been tested is probably a good reason why it will end up down the drain. The main reason you are probably not testing is that you have no idea how to do it on anything beyond FizzBuzz, let alone in a TDD workflow. Testing in Data Science is hard, but do not kid yourself: it is doable and should be done. If anything, the specific hardships of Data Science should make us yearn for already-proven solutions that increase software reliability and decrease our cognitive burden. But I guess some people like their legacy projects served early in the morning with entire chunks of burnout in them.\nClearly, the “Dev” culture has not yet been imported into the Data Science world, and most of us definitely missed the DevOps train. Yet, people who have jumped on it are out there. They deploy 5 times a day to production, use canary releases after training 10 models concurrently on perfect copies of their prod environment, after a simple commit-push. None of these steps can be reached without strict automated testing or a strong self-discipline, such as not touching production with greasy data scientist fingers. We should strive for better quality standards if we ever want to be considered serious software engineers.\nIn the end, mastery is what we need. With this demonstration, I hope you are now as absolutely convinced as I am of the imperious necessity of importing good testing strategies into Data Science. Otherwise, it’s better if you and I never find ourselves in the same room, because I will definitely reach for that shovel.\n\nGood Reading\n\nModern Software Engineering - David Farley - ISBN: 9780137314867\nExtreme Programming Explained: Embrace Change - Beck, Kent; Andres, Cynthia - ISBN 10: 0321278658 / ISBN 13: 9780321278654\nTeam Topologies - Matthew Skelton; Manuel Pais - ISBN: 9781966280002\nAccelerate: The Science of Lean Software and DevOps - Gene Kim, Jez Humble, Nicole Forsgren - ISBN 10: 1942788339 / ISBN 13: 9781942788331\nTest Driven Development: By Example - Kent Beck - ISBN: 9780321146533"
  },
  {
    "objectID": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html",
    "href": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "",
    "text": "That’s it! Three times! The third time is the charm, as they say!\nIt has been three times, THIS VERY WEEK, that I’ve been pulled into a meeting because someone had a performance problem with their pandas code. Each time it’s the same problem: some kind of genius decides to remove all safety belts, starts hacking and typing until their code regurgitates something. Then that person just hopefully and magically wishes that it will produce the expected outcome. Well, the outcome is me being called, and I’m deeply aggravated:\nIF YOU HAVE FOR LOOPS IN YOUR PANDAS CODE, YOU ARE WRONG!\nThere, I said it…"
  },
  {
    "objectID": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#the-not-so-good",
    "href": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#the-not-so-good",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "The not-so-good",
    "text": "The not-so-good\nThe first time was a young data analyst who produced a notebook to perform some anomaly detection on curves. It was running slow even though it was running on a GCP machine that could probably crack half of the Internet passwords in an hour. The first rookie mistake was trying to plot about 500 examples of curves with about 5000 points per curve because the sampling rate is under a millisecond. I mean, plotly is a great library, no doubt about it, but that is a lot of things to print on your screen.\nThe machine breathes a sigh of relief…\nBut the signs of abuse were deeper. I will overlook the code blocks with a width on screen of 700 characters because everyone loves to scroll right and left (Please just run ruff on your notebooks). I will purposely ignore the function that does data pre-processing AND plotting, for now… But I cannot turn a blind eye to the double-nested for loop with print statements every 10ms and in-place modification of your gigantic dataframe that propels your machine into locked-in syndrome.\nAlright, nothing that can’t be fixed with a little bit of explanation and starting from the basics: down-sampling large data, method chaining in pandas, and no for loops, please."
  },
  {
    "objectID": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#the-bad",
    "href": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#the-bad",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "The bad",
    "text": "The bad\nThe second time felt awfully similar. I got called for support by a senior contractor because their code on our MLOPS pipeline in Vertex AI had crashed, apparently due to an out-of-memory error. For those who don’t know, Vertex AI is a GCP service that allows you to run machine learning jobs, using Kubernetes under the hood, so basically you can scale up the machine running your job quite a bit. Any data scientist would think logically that the thing crashed during the training, but it was the prediction part that had crashed, AFTER RUNNING FOR TWO DAYS!\nWhat kind of unholy job are you guys running there? Ah, you’re running a PyTorch model to classify texts, great! Are you trying to classify the whole internet for it to take this long? What’s the size of your dataset? 20K text samples and the whole size is around a few 100s MB? Mother… I don’t know how you’re doing it, but you really managed to mess this one up. In the absolute worst case, this should run in a matter of hours, not days!\nAlright! Time to deep dive and see what ungodly horrors lie at the bottom of this…\nMy good sir, I thank you for creating functions, but yours are 50+ lines long, take 10 different arguments, and modify things in place… I hope you have tests for this (who am I kidding) … Anyway, where is your logic for the prediction? … Within a class with snake-case naming, not the worst thing I will see today, I am sure… Ah, this is the meat of all, the “predict” method… And…\nNO! GOD! NO! PLEASE! NO!\nYou sir, are guilty of the sin of abusing the for loop and pd.concat pattern. Don’t tell me you are redoing this for each line of your dataframe? Of course you are! Are you even aware that for each line you are basically writing a whole new dataframe, which according to you is several 100s MB, that contains just an extra prediction compared to the old one, then doing an in-place operation to replace the old one, and you are wondering why your code takes this long? You’re not even wondering because it did not occur to you that such a job running for several days was a horrible mistake. Oh, and by the way, do you know that you have a triple-nested for loop here in this sub-function of your method “predict”?\nYeah, this thing is going places."
  },
  {
    "objectID": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#there-is-nothing-sacred-anymore",
    "href": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#there-is-nothing-sacred-anymore",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "There is nothing sacred anymore",
    "text": "There is nothing sacred anymore\nThe pinnacle of incompetence was definitely reached by a few members of a consulting firm, you know one of the BIG players that sell their meat “highly trained experts” for a modest 2000-4000 dollars a day. Same scenario, I get added to a meeting whose objective was to determine if the MLOPS pipeline we developed can accommodate scaling up the model they developed because currently their prediction is taking 3-4 hours and they have only implemented 1 of the 100 possible cases. Obviously, it will be very difficult to run the job for all cases within a day. Obviously, they are using the largest machine they can for their Vertex AI job. Obviously, they are trying to say that their “high-end” solution is in danger, although we paid a hefty price for it. Obviously, it’s about blame shifting. Sadly, for them, I already know that they are full of shit.\nThe first reason is that a few days ago, I had already heard of one of their “great” engineers reaching for help because his highly efficient unit tests were not working in the CI because he did a bunch of calls to Google Storage and BigQuery in them. Calling the database in unit tests, genius… But the second and most important reason why they are full of shit is my secret weapon. It is because our generic MLOPS pipeline does quite a bit between jobs but almost nothing within the job per se, just a little bit of I/O at the start and the end, everything else is just the logic implemented by the “customer” data scientist. In other words, if things go sideways during the job, then you sir, you are the problem! Sorry, I’m not sorry.\nLet’s speed up the next 25 minutes of the discussion:\nSo, let’s see the logs… Apparently, so you have about 100 configurations and for every one of them you’re spending many minutes in method X doing some pre-processing… let’s pull up the corresponding code… And…\nNO! GOD! NO! PLEASE! NO!\n\n1default_dict_of_df : dict[str, list[pd.DataFrame]] = ...\n\ndata_per_provider = {\"total\": [], **default_dict_of_df}\n\nfor provider in dataset[\"provider\"].unique():\n    provider_df = dataset[dataset[\"provider\"] == provider]\n\n    data_per_provider[\"total\"].append(\n2        generate_feature_time_series(\n            provider_df, date_index, provider\n        )\n    )\n    for feature in list_of_feature:\n3        for value in provider_df[feature].unique():\n            data_per_provider[feature].append(\n4                generate_feature_time_series(\n5                    provider_df[provider_df[feature] == value],\n                    date_index,\n                    provider,\n                    feature=feature,\n                    value=value,\n                )\n            )\nprovider_stocks_dfs = {\n    feature: pd.concat(data_per_provider[feature], axis=0)\n    for feature in [\"total\", *list_of_feature]\n}\n\n1\n\nJust the type of this thing sounds already like a bad idea\n\n2\n\nTwo pivot and two reindex operations in there. Bye bye performance…\n\n3\n\nTriple for loop nesting, aka. the speeding ticket of computer!\n\n4\n\nAgain, two pivot and two reindex operations in there. Bye bye performance…\n\n5\n\nNotice calling the same function with a different granularity, pure art!\n\n\nYeah, triple-nested for loops, combined with apply-pivot-append operations with concatenation in the end, if this was figure skating you would probably score lots of points. Too bad you are trying to do data science.\nIn the end, a team of so-called “expert” consulting engineers wrote something so egregious it threatened the whole project. Any junior writing such a thing should be taught better. None of them bothered to check the logs to investigate the issue. Oh, and by the way, the huge machine they were using was running at 20% of CPU and RAM load all the time, they did not check that either! Oh and they did the same mistake in two places within the codebase, so now we have to wait twice as much.\nI guess shipping garbage that barely works is definitely a good way to get your contract renewed and be paid to continue maintaining it… But this is a different story."
  },
  {
    "objectID": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#final-thoughts",
    "href": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#final-thoughts",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "Final thoughts",
    "text": "Final thoughts\nSo please kids, do not abuse your pandas. Keep them safe from the bad influence of for loops. Believe me, it’s much easier to make your code not suck rather than spending days trying to optimize a clunky piece. I fully agree here with what Casey Muratori says about non-pessimization and this needs to be heard everywhere:\n\nNon-pessimization, it’s simply not introducing tons of extra work for the computer to do!"
  },
  {
    "objectID": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#disclaimer",
    "href": "posts/2025/2025_04_13_your_pandas_code_is_bad/index.html#disclaimer",
    "title": "Your Pandas Code Is Bad And You Should Feel Bad!",
    "section": "Disclaimer",
    "text": "Disclaimer\nThe style of this post may remind some of you of the comrade Ludicity and I must confess I am a deep fan and an active reader of his work.\nThough I tried not to imitate his style too much, this post MUST be a strong rant because some things need to be said out loud. Also, because of my own swearing nature and my shortcomings in writing, I doubt that I managed efficiently to do so. Ludicity, if you’re reading this, I apologize deeply."
  },
  {
    "objectID": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html",
    "href": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html",
    "title": "A Case Defense Of Casey Muratori: Non-Pessimization Vs. Optimization",
    "section": "",
    "text": "Sorry, I am not sorry about the orthographic pun.\nRegardless, my inferior brainstem feels like babbling about a few videos by Casey Muratori on the excellent YouTube channel Molly Rocket, namely:\nWhile Casey has quite a few hot takes, we will speak only here about the “optimization” part and not some other topics like: “Code is running much slower nowadays than it should.” Mainly because I’m not a developer, and my very limited experience with compiled languages makes my rotten brain unable to form a judgment here.\nThe main conundrum is this: There is a finite upper limit for the optimization of a given piece of code, but there is no lower limit to how slow and crappy you can make it.\nSo bottom line, it is usually easier, faster, and more achievable to avoid putting garbage in the code rather than trying to optimize the shit out of it. Don’t make things bad, m’kay?\nNot convinced? I’ll give you a clear practical example to other moronic data scientists like me that have never seen the inside of a compiler. Look at BigQuery: this stuff is pure magic for processing Petabyte-scale datasets, with almost infinite scalability, and infinite parallelization. Yet, with the right amount of criminal intellectual deviance, you can still make it as slow as a blind horse overdosing on Xanax that you shot in the leg to give it a good head start.\nNow that I have your ameboid brain on board, let’s go to the main topic:"
  },
  {
    "objectID": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#the-definition-of-non-pessimization-and-optimization",
    "href": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#the-definition-of-non-pessimization-and-optimization",
    "title": "A Case Defense Of Casey Muratori: Non-Pessimization Vs. Optimization",
    "section": "The Definition Of Non-Pessimization And Optimization",
    "text": "The Definition Of Non-Pessimization And Optimization\nAccording to Casey, there are 3 different sides to what people call optimization:\n\nOptimization:\n\nPrecisely measuring, comparing, and testing various properties of your code (speed, throughput, latency…), through various implementations if possible done on hardware/context similar to its final execution.\n\nDifficult, time-consuming, few people can do it, only useful for very key components of your system.\nNon-Pessimization\n\nNot introducing tons of crap work for the computer to do.\n\nAccessible to most people with moderate knowledge of a given paradigm/language, applicable almost everywhere, fast.\nI will personally add: it usually makes the code more readable to humans as well…\nFake Optimization\n\nTrying to do No. 1 without measuring, rigorously testing, or skills.\n\nTypically happens when people are not able to do step 2 and jump directly to step 1. This usually makes things worse for everyone.\nThink people that try to strap multiprocessing/multiprocessing on your average code dumpster fire because “time will SURELY be divided by 4 if I run my program on 4 cores”…\n\nSo we have to focus on things that are reasonable.\n\n\n“But Dr. Rants, how can I possibly know when should I do No. 1, 2?”\n“WRONG! No. 1 does not exist for you! For you, it’s myth! Otherwise, you would not need to ask the question.” It’s plain simple really. If you’re trying to do Optimization on a code that’s bad because you just did not understand the concept of non-pessimization, then you’re back to the whack-job No. 3! For you only one thing matters:\nNON-PESSIMIZATION!”\n\n\nThen comes the Real question: How can I make my code not suck?\nCasey has a pretty simple answer:"
  },
  {
    "objectID": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#you-should-know-a-bit-of-assembly",
    "href": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#you-should-know-a-bit-of-assembly",
    "title": "A Case Defense Of Casey Muratori: Non-Pessimization Vs. Optimization",
    "section": "You Should Know A Bit Of Assembly",
    "text": "You Should Know A Bit Of Assembly\nThis is one of the probably hottest takes of Casey, i.e. his focus on having us learn a bit of assembly. I will paraphrase but the general idea is:\n\nYou should learn assembly and be able to view your compiled program in assembly.\nThis way you will be able to actually see and understand HOW your program is doing what you instructed it to do.\nThe instruction set is limited and thus quite simple in comparison to other languages, especially high-level ones.\nIt makes a tremendous difference to be able to peek under the hood to develop an understanding of how the computer operates because you removed the middleman.\nEven in high-level languages like JavaScript and Python, it is possible to have a look at the low-level code and it’s not that difficult.\n\nIn short, it’s not too difficult, useful, and you really understand how things run.\nI have to admit that my lazy ass cheeks are happily clapping in the realm of high-level languages. I am doing data science, where most of my days are spent within a DSL of Python: polars/pandas or in the happy declarative land of SQL. And behold! Once in a while on a hot day, I do a few network calls, yeah real thug life! Clearly, I am not the target audience when everything needs to be rewritten from scratch.\nOnly a madman would ask me to code a dictionary or an array manipulation framework from scratch! Unless you really want your product to be a gaping security hell-hole, with nice entire chunks of memory leaks? No? I thought so! Anyway, if such a set of basic things are not part of the standard library or the package ecosystem, it is of little use/appeal to me. This is why I am more interested in Rust/Go with their promise on performance while offering some higher-level interface with little overhead.\nBut enough digressions, in the vast lands of SQL and data warehouses, each with its own engine, translating your code to assembly makes no sense at all. I am nevertheless genuinely curious to see if the large binary files for pandas/polars could be relatively “simple” as Casey tends to portray it. And I would love to see the instruction sets and general layout there.\nHowever, beyond the dark fearful “assembly” thingy, there is pure gold. Assembly here is just a means to obtain an understanding of how the computer operates and how your language of choice will choose to run certain operations.\nThis is where I am in complete agreement with Casey and where we can really witness:"
  },
  {
    "objectID": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#the-staggering-difference-when-people-have-a-general-idea-of-how-their-code-runs",
    "href": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#the-staggering-difference-when-people-have-a-general-idea-of-how-their-code-runs",
    "title": "A Case Defense Of Casey Muratori: Non-Pessimization Vs. Optimization",
    "section": "The Staggering Difference When People Have A General Idea Of How Their Code Runs",
    "text": "The Staggering Difference When People Have A General Idea Of How Their Code Runs\nI have witnessed firsthand that in the DS world a huge discriminator for not sniffing glue, is simply having a vague mental model of how the code will run. Let’s take your favorite DS package: pandas. Let’s not delve into its broken API, and focus on the pure execution of things. If I run a motherfucking Cmd + Shift + F to search on your whole code base, how many .apply() and .iterrows() will I find? Don’t lie to me and don’t lie to yourself! I’m not even talking about your horrible inplace=True, I am sure there are some guilty stains left. Continue with this kind of stunts, and your next encounter with me will make an episode of Happy Tree Friends look like a casual garden party.\nI see way too many people exhibiting a blatant dead-fish stare whenever you mention the word vectorization for operating on a dataframe, and blasting .apply calls to process text in every corner in the code because apparently typing .str is too complicated.\nIn SQL and your average distributed database, if you are treating a few GBs and it takes minutes, you, my friend, are the source of your own sorrow. But at least, you have the excuse of being able to time your code properly. This strengthens even more the need for non-pessimization, precisely because No. 1 optimization is even more difficult!\nBesides, you don’t want to be the person that writes SQL for 2 years before asking: “Wait a minute, does it change anything if my database is row- or column-oriented?” You think I’m joking? Most juniors I have seen, including myself, only get a brief exposure to SQL, land a job, and then you better pray to all the gods in creation that a randomly passing senior will explain to them what life is all about. Or else they will gut the data pipeline inside-out before turning into agents of Chaos!\nAnyway, you can start by removing all the ORDER BY that basically rhyme with bottleneck, or your average SELECT DISTINCT which is only there to hide your shameful joins or a non-existent primary key, and while you’re at it, learning about window functions won’t kill you."
  },
  {
    "objectID": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#you-make-it-sound-easy",
    "href": "posts/2025/2025_06_05_non-pessimization/non-pessimization.html#you-make-it-sound-easy",
    "title": "A Case Defense Of Casey Muratori: Non-Pessimization Vs. Optimization",
    "section": "You Make It Sound Easy",
    "text": "You Make It Sound Easy\nWell, at least not terribly difficult… Still, it requires at least a few cardinal skills that are unattainable to the average LinkedIn shitposter compulsively liking all posts labeled: “17 core python functions you should know.” By that, I mean the capacity to perform a Google/LLM search with at least one brain cell engaged, the magic power of reading the documentation without experiencing an anxiety/boredom attack, and probably the most seldom of the three, a little shred of curiosity for what is the best way to use your current tool, framework, or DSL.\nC’mon, let’s talk real for a second! When is the last time you added the words “best practices” to your search? Instead, you started hacking code as if your product owner was trying to pin down JIRA tickets between your butt cheeks. How many fucking hours do you spend each week in pandas/SQL? Isn’t it worth spending at least 5% of this time to learn how to better use the tool? No need to answer, that’s called a rhetorical question.\nLearn a thing or two about your CPU, read something about the engine of your database, do a lot more than that if you use GPUs, and for the sake of everyone’s sanity do a lot more than that if your model even remotely interacts with humans…"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DS Rants Main Page",
    "section": "",
    "text": "The Angry Guide To Pandas Code - Applying Vengeance (Part 2)\n\n\n\ndata science\n\npandas code\n\nnon-pessimization\n\nperformance\n\n\n\n\n\n\n\n\n\nDec 5, 2025\n\n\nDS Rants\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\nY’all Motherfuckers Need Tests, So Stop Making Lazy Excuses\n\n\n\ndata science\n\nsoftware engineering\n\ntests\n\nbest practices\n\n\n\n\n\n\n\n\n\nNov 20, 2025\n\n\nDS Rants\n\n13 min\n\n\n\n\n\n\n\n\n\n\n\nY’All Motherfuckers Need Tests\n\n\n\ndata science\n\nsoftware engineering\n\ntests\n\nbest practices\n\n\n\n\n\n\n\n\n\nNov 20, 2025\n\n\nDS Rants\n\n18 min\n\n\n\n\n\n\n\n\n\n\n\nA Case Defense Of Casey Muratori: Non-Pessimization Vs. Optimization\n\n\n\ndata science\n\nperformance\n\nCPU\n\noptimization\n\n\n\n\n\n\n\n\n\nJun 5, 2025\n\n\nDS Rants\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\nThe Angry Guide To Pandas Code (Dummy Edition, Part 1)\n\n\n\ndata science\n\npandas code\n\nnon-pessimization\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\nDS Rants\n\n15 min\n\n\n\n\n\n\n\n\n\n\n\nI Will Not Attend Your Pre-Pre-Pre-Sprint-Planning Meeting\n\n\n\nsoftware engineering\n\nfake agile\n\nscrum clusterfuck\n\nmeetings\n\n\n\n\n\n\n\n\n\nApr 20, 2025\n\n\nDS Rants\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\nYour Pandas Code Is Bad And You Should Feel Bad!\n\n\n\npandas code\n\nperformance\n\nproduction\n\n\n\n\n\n\n\n\n\nApr 13, 2025\n\n\nDS Rants\n\n9 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025/2025_04_20_pre-pre-sprint-planning/index.html",
    "href": "posts/2025/2025_04_20_pre-pre-sprint-planning/index.html",
    "title": "I Will Not Attend Your Pre-Pre-Pre-Sprint-Planning Meeting",
    "section": "",
    "text": "Dear Product Owner / Manager / Team Leader,\nMy heart was filled with pain and sorrow when I noticed you sent an invitation for a pre-sprint planning meeting to be held on the day before the actual event. I am deeply saddened to inform you that I will not attend any such events, on any occasion, ever.\nHere I must stand my ground because I know what lies ahead, even if you don’t know it yourself. You will speak about the 20-minute sprint planning meeting being too short to write and select the tickets, you will say that there is a need for larger synchronization and cooperation within the team, you will claim that the pre-sprint-planning event will only last 10 minutes. You are lying to us, to the world, and to yourself, but you don’t know it yet.\nYou are suffering from a clear case of chronic scrumitis and sadly the only reasonable treatment is abstinence, whether it is of your own will or forced. You are trying to create more of the scourge that plagues software engineering, you have heard of it, but did not think much of it. It has crept through your own week, preventing you from doing anything useful, and now you’re attempting to cure this evil by creating more of it: Meetings!\nI did not say much previously because, all things considered, you were pretty reasonable. Our sprint planning meetings usually lasted for their originally planned 20 minutes, sometimes a bit more but not too much. You insisted on doing 30-minute retrospectives every two weeks, and I did not complain even though it was pretty much pointless. Very little value came out of it, but you looked so happy seeing the engineers getting together and doing the “team building” activities you so proudly crafted. You even had good things for you when you tried to keep daily meetings within their planned 10-minute bounds, I really commend you for that feat! Sadly, you’re going down a dreaded path and you don’t know it yet.\nI have seen what will happen, you will round up a few consenting engineers who quite like you and bring them to their doom (Fly! You fools!). They will fall for the dumb excuses you make, the nice jokes you tell, or just yield to peer pressure and fold into the line… You have already betrayed them, but you don’t know it yet.\nThat original sin that was supposed to “last 10 minutes, yeah, I swear on my mother’s life… I also have things to do… you can trust me…”. Well now it has been 40 minutes and far from over. I see the despair on the faces of the same engineers I could not save from your grasp, who are now in the deepest pit of hell-scrum. I see their eyes drift away with a thousand-mile stare as you suck the energy out of them, and soon you will ask: “How many points should we give for this story?”. You’re dangerous but you don’t know it yet.\nYou could have chosen many other paths. You could have selected carefully a few customers that you would have brought to the engineers for them to interact directly. You could have fought for these same engineers and cleared their calendar from the filth imposed by other fools, for them to start pair-programming and actually doing work. You could have let go of that little feature and its stupid story points to decide instead to write tests so that your software does not turn into a pile of crap in 6 months. You could have read a few books written by experts who faced the same issues before and found working solutions. You’re ignorant but you don’t know it yet.\nThe work could have been managed very differently. Who said that you were not supposed to take up another task during the week if you’re done early with the first? Who said that it’s not possible to add a new task on the board if something comes up in the middle of the work? You never said such things, yet they probably came to your mind in a malformed way. You probably imagined deep down that your engineers were not “stream-aligned”, “business-oriented”, hard-working enough, and you imagined they needed actual supervision, thus you started patronizing your team but you don’t know it yet.\nAnd soon you will find that this pre-sprint-planning meeting does not produce the results you hoped for. Your team does not deliver enough features, you still think they need better synchronization, more teamwork, and stronger engagement. So you will think really hard about it because you’re not dumb, because you have faced issues before and things kinda worked out, we just need more of all those things. You will work, fight, overcome this hardship and you will do it:\nYou will add another meeting to prepare for the pre-sprint-planning meeting.\nNow, you know.\nSincerely\nYour Friendly Neighborhood Data Scientist"
  },
  {
    "objectID": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html",
    "href": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html",
    "title": "The Angry Guide To Pandas Code (Dummy Edition, Part 1)",
    "section": "",
    "text": "Alright! You apparently stumbled by accident on this page after wrongly clicking on the banner: “There are single people in your area”.\nWell now, you have at least one chance to do something productive today! So put on your best nerd reading glasses and listen.\nToday, you’re going to learn how to write pandas code that does not look like the experimentation of a toddler smashing on the keyboard because it prints stuff on the screen. You’re going to move one inch closer to the golden age of civilization. So buckle up, stop thinking for your own good, and read carefully!\nActually, before we even start with code… NO FUCKING FOR LOOPS!\nAnyway, we will start with the usual suspects, and a stupid-ass dataframe:\nLooks stupid enough? Good, I don’t want you distracted one bit by any type of complexity. Imagine you have been working on it. What you’re probably seeing in your average notebook probably looks a lot like the following. I am sure of it because I captured this in the wild:"
  },
  {
    "objectID": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#the-original-sin-of-pandas",
    "href": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#the-original-sin-of-pandas",
    "title": "The Angry Guide To Pandas Code (Dummy Edition, Part 1)",
    "section": "The Original Sin Of Pandas",
    "text": "The Original Sin Of Pandas\n\nTypical Bullshit OneTypical Bullshit Two\n\n\n# fmt: off\nweekly_chart_df = original_df.reset_index()\nweekly_chart_df.rename(columns={0 : 'category_A', 1 : 'category_B', 3 : 'category_C'}, inplace=True)\nweekly_chart_df.year_week = weekly_chart_df.year_week.astype(str)\nweekly_chart_df['total_produced'] = weekly_chart_df.category_A + weekly_chart_df.category_B + weekly_chart_df.category_C # total number of produced goods\nweekly_chart_df['total_tested'] = weekly_chart_df.category_B + weekly_chart_df.category_C # total number of tested goods\nweekly_chart_df['prop_tested'] = np.round(weekly_chart_df.total_tested / weekly_chart_df.total_produced *100,2) # proportion of goods tested\nweekly_chart_df['prop_category_B'] = np.round(weekly_chart_df.category_B / weekly_chart_df.total_produced *100,2) # proportion of category_B\nweekly_chart_df['prop_category_C'] = np.round(weekly_chart_df.category_C / weekly_chart_df.total_produced *100,2) # proportion of category_C\nweekly_chart_df = weekly_chart_df[weekly_chart_df[\"total_tested\"] &lt; 5]\n# fmt: on\n\n\n# fmt: off\ndf = original_df.reset_index()\ndf.rename(columns={0 : 'category_A', 1 : 'category_B', 3 : 'category_C'}, inplace=True)\ndf.year_week = df.year_week.astype(str)\ndf['total_produced'] = df.category_A + df.category_B + df.category_C # total number of produced goods\ndf['total_tested'] = df.category_B + df.category_C # total number of tested goods\ndf['prop_tested'] = np.round(df.total_tested / df.total_produced *100,2) # proportion of goods tested\ndf['prop_category_B'] = np.round(df.category_B / df.total_produced *100,2) # proportion of category_B\ndf['prop_category_C'] = np.round(df.category_C / df.total_produced *100,2) # proportion of category_C\ndf = df[df[\"total_tested\"] &lt; 5]\nweekly_chart_df = df\n# fmt: on\n\n\n\nLooks familiar? Do you feel this conundrum where either your names are long and things are a pain to write? Or you abuse the ignominious df name all over the place and everything is called the same?\nWell, I am sorry to tell you that both examples up there are pure garbage! Actually, I am not even sorry. This stuff has been vomited by someone without any consideration for their fellow humans. You don’t believe me?\nDid you by any chance happen to notice the little cabalistic signs around the code: fmt: off / fmt: on? Do you know what it means? It means I have to purposefully block my formatter from touching this pile of intellectual dump, otherwise, it becomes so unreadable that its ungodly nature becomes clear to all. Let me prove it to you:\nweekly_chart_df = original_df.reset_index()\nweekly_chart_df.rename(\n    columns={0: \"category_A\", 1: \"category_B\", 3: \"category_C\"}, inplace=True\n)\nweekly_chart_df.year_week = weekly_chart_df.year_week.astype(str)\nweekly_chart_df[\"total_produced\"] = (\n    weekly_chart_df.category_A\n    + weekly_chart_df.category_B\n    + weekly_chart_df.category_C\n)  # total number of produced goods is the lot\nweekly_chart_df[\"total_tested\"] = (\n    weekly_chart_df.category_B + weekly_chart_df.category_C\n)  # total number of tested goods is the lot\nweekly_chart_df[\"prop_tested\"] = np.round(\n    weekly_chart_df.total_tested / weekly_chart_df.total_produced * 100, 2\n)  # proportion of goods tested on the lot\nweekly_chart_df[\"prop_category_B\"] = np.round(\n    weekly_chart_df.category_B / weekly_chart_df.total_produced * 100, 2\n)  # proportion of category_B in the lot\nweekly_chart_df[\"prop_category_C\"] = np.round(\n    weekly_chart_df.category_C / weekly_chart_df.total_produced * 100, 2\n)  # proportion of category_C in the lot\nweekly_chart_df = weekly_chart_df[weekly_chart_df[\"total_tested\"] &lt; 5]\nUnderstood? Now that the pretended pseudo-manual structure is gone, there is nothing left but chaos? So do yourself a favor, just install ruff or black, because if you write something, format it, and it looks worse, there is a 99.999% chance that what you wrote should be covered with Napalm and set on fire.\nNow that we agree on the horror it actually is, it is time to fix it."
  },
  {
    "objectID": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#how-to-see-the-light-in-darkness",
    "href": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#how-to-see-the-light-in-darkness",
    "title": "The Angry Guide To Pandas Code (Dummy Edition, Part 1)",
    "section": "How To See The Light In Darkness",
    "text": "How To See The Light In Darkness\nFirst, you’re going to remove ALL inplace=True from your whole code base! Did I stutter? I said all of them! This wretched abomination should have never seen the sunlight:\n\n\n\nFor those who are to lazy to click\n\n\nUnfortunately, pandas is almost 20 years old, and mistakes were made during the young years. Sadly enough, this mistake still haunts us to this day because some people are clearly not able to perform a simple Google search. We are still stuck with this because removing it would break half of the world’s code bases, because of people like you. People who think they are being clever, when in fact they’re just performing a major enshittification of the code at a large scale.\nNo inplace=True EVER! I mean it!\nSo, the first line becomes:\nweekly_chart_df = original_df.reset_index().rename(\n    columns={0: \"category_A\", 1: \"category_B\", 3: \"category_C\"}\n)\nSee? It’s not beautiful yet but it is certainly a step in the right direction?\nThe next thing you will do is transform the astype call to use it at the dataframe level, in the shape: astype({\"column\": &lt;type&gt;}), because we are not beasts. Not so goddamn fast you inconsequential stardust residue! I am sure you were about to type something ludicrous like:\np_chart_weekly = p_chart_weekly.astype({\"year_week\": str})\nYou, you’re trying to make me mad, aren’t you? Put that shit with the previous calls, and lose the name of the dataframe you don’t need it.\nweekly_chart_df = original_df.reset_index().rename(\n    columns={0: \"category_A\", 1: \"category_B\", 3: \"category_C\"}\n).astype({\"year_week\": str})\nSee this shit? This is called method chaining, now repeat after me: method chaining. This is the sacred beauty that was sent upon us mere mortals to allow writing data transformations that do not look like complete crap. You will only write your pandas this way from now on! Did you hear me? And I will show you later how other languages have managed to do way better than Python and pandas in that regard. And these people were rightfully laughing at your clown ass, when you were happy writing some stupid shit like df = df.some_fucking_transformation Do you start to understand? This is why I forbade your sorry brain to ever use the option inplace=True, because it breaks the… you’re goddamn right, method chaining!\nFrom this point, you are one keyboard shortcut away from poetry. You don’t believe me? Did you install an automatic code formatter as I so generously told you earlier? You, infected chromosomal deficiency! Ok, it appears you need another round of scolding. You need a formatter like ruff or black because your inept brain will not be able to remain consistent to format your whole project in a standard way. You don’t want to waste any part of your limited brain power on stupid shit like this. Just use the goddamn tools! Some talented person might pretend: “It looks prettier when I do it myself, and I manage to remain completely consistent across my whole project”, but you are not some person…\nweekly_chart_df = (\n    original_df.reset_index()\n    .rename(columns={0: \"category_A\", 1: \"category_B\", 3: \"category_C\"})\n    .astype({\"year_week\": str})\n)\nSee how beautiful this is? Now you’re starting to believe. Ok, it’s time to introduce you to a concept you have never encountered: (). Those are called parentheses, and they are your new best friend!\nWhenever you want to write a transformation, BEFORE you even type the name of your stupid dataframe, which is probably stupid df anyway, the absolute FIRST THING you will do is open a pair of parentheses. Why? Because each time you’re done with one call for a transformation, you can just press enter, the code will auto-indent, and you can continue whatever sorry exploration you’re attempting. Is it so much better than your typical awful backslashes, don’t lie, I know you have been abusing them…\n(\n    data\n    .transformation_1()\n    .filtering()\n    .create_columns()\n    .groupby()\n    .agg()\n)\nUnderstood? Good!"
  },
  {
    "objectID": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#the-unavoidable-truth-of-data-transformation",
    "href": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#the-unavoidable-truth-of-data-transformation",
    "title": "The Angry Guide To Pandas Code (Dummy Edition, Part 1)",
    "section": "The Unavoidable Truth Of Data Transformation",
    "text": "The Unavoidable Truth Of Data Transformation\n“But, sir Rants, how can I possibly continue chaining when I want to create a column?” you ask?\nHow about a good ol’: Read The Fucking Manual? What? Too old school? Ok, you can probably ask your favorite ChatGPT / copilot / gemini, or whatever shit was the weekly trash hype on LinkedIn this week… But you’re going to do something, you’re going to treat that piece of shit-sorry-for-lying-unreliably-hallucinating LLM like the moronic tool it is! You’re going to prompt it rightfully by explicitly writing:\n\nUsing method-chaining, and only method-chaining in pandas, how do I …\n\nAnd you’re going to insult every generation of GPU that was used to breed that useless LLM until it regurgitates what you want! And do you know why? Because 95% or more people code like you. So the training dataset for good pandas code used by the LLMs is at least an order of magnitude smaller than the crap we have been seeing so far.\nRegardless, you should end up with a bingo: .assign(). But you’re going to read the documentation, no matter what:\n\n\n\nBehold, the glorious assign method!\n\n\nDo you notice what I subtly highlighted for your limited brain capabilities? The first use case is a dictionary associating a string with a Callable. For those who skipped 7th grade English, it means a plain stupid function!\nNow read the underlined sentence! Read it again! Now look at the examples. Read it again! It means if you use a function in your kwargs, pandas will automatically call this function on the dataframe that generated the assign call, and said dataframe will be the first argument! Do you understand how powerful this is? It means that you just gained a shortcut to reference your current dataframe! How, you ask? Do I really have to explain everything to your sorry ass?\nHave you heard of the keyword lambda? What are those? You’re goddamn right: functions! And what do you use functions for? Referencing the current dataframe!\nDing! Ding! Ding!\nweekly_chart_with_proportions = (\n    original_df.reset_index()\n    .rename(columns={0: \"category_A\", 1: \"category_B\", 3: \"category_C\"})\n    .astype({\"year_week\": str})\n    .assign(\n        total_produced=lambda df: df.category_A + df.category_B + df.category_C,\n        total_tested=lambda df: df.category_B + df.category_C,\n        prop_category_B=lambda df: (df.category_B / df.total_produced * 100).round(2),\n        prop_tested=lambda df: (df.total_tested / df.total_produced * 100).round(2),\n        prop_category_C=lambda df: (df.category_C / df.total_produced * 100).round(2),\n    )\n)\nSee how much NICER this looks? This is starting to look like someone who knows their job. You have subject, verb one, verb two plus complements, and verb three plus complements. I bet your over-complexing anxiety-riddled brain would have never trusted me if I told you to write nice efficient code, you just need 6th-grade grammar!\nOh, and do you know the AWESOME BONUS I just gave you for free? No more SettingWithCopyWarning. EVER! PERIOD! Because we are never modifying the dataframe in-place, we’re always returning a new instance. And that’s totally fine because we don’t have tons of stupid-ass intermediate variables like df, df1, df2, temp, temp2 that serve no purpose outside supporting your limping transformation logic and eating up all your RAM.\nOh, and by the way, I took the liberty to remove your stupid calls to numpy because guess what? 99% of the time, there is a pandas equivalent that is within reach by just pressing a dumb dot ., so be intelligently lazy!\nNow the last part, let’s take care of the filter. You were going to keep the original thing, weren’t you?\nweekly_chart_df = weekly_chart_df[weekly_chart_df[\"total_tested\"] &lt; 5]\nThere are 2 ways to filter dynamically a dataframe in pandas. One is using the cursed sin of arbitrary string execution, and the other provides autocompletion and syntax highlighting! Clearly, only a moronic ape high on cocaine, or an agile coach brainwashed on scrum/SAFe bullshit, same difference, would choose the first over the second. Therefore, we will never mention the first abomination again!\nAlright, back to the code! Do you know what those brackets [] stand for? If you say getitem you’re on the right track, but in pandas, they are used to call the function loc[]. Yes, squared-brackets here because the pandas API is fundamentally broken for those who experienced at least another language. But we will come back to this in another post.\nFor now, just read the documentation for the loc[] method. Is there anything that looks familiar? For those with a lightbulb that requires a constant perfusion of coffee to produce an ersatz of a thought, we are looking for a way to filter the current dataframe. Did you see it? The word callable? Are you beginning to understand? Everything I told you so far serves a glorious purpose!\nNow, we can finish our business here:\nweekly_chart_with_proportions = (\n    original_df.reset_index()\n    .rename(columns={0: \"category_A\", 1: \"category_B\", 3: \"category_C\"})\n    .astype({\"year_week\": str})\n    .assign(\n        total_produced=lambda df: df.category_A + df.category_B + df.category_C,\n        total_tested=lambda df: df.category_B + df.category_C,\n        prop_category_B=lambda df: (df.category_B / df.total_produced * 100).round(2),\n        prop_tested=lambda df: (df.total_tested / df.total_produced * 100).round(2),\n        prop_category_C=lambda df: (df.category_C / df.total_produced * 100).round(2),\n    )\n    .loc[lambda df: df.total_tested &lt; 6]\n)\nNow we have finally entered the age of civilization! Are you feeling proud of yourself? Not so damn fast! Time to see what other people in the R-world have been doing:\nweekly_chart_with_proportions &lt;- original_df %&gt;%\n  rename(\n    category_A = `0`,\n    category_B = `1`,\n    category_C = `3`\n  ) %&gt;%\n  mutate(\n    year_week = as.character(year_week),\n    total_produced = category_A + category_B + category_C,\n    total_tested = category_B + category_C,\n    prop_tested = round(total_tested / total_produced * 100, 2),\n    prop_category_B = round(category_B / total_produced * 100, 2),\n    prop_category_C = round(category_C / total_produced * 100, 2)\n  ) %&gt;%\n  filter(total_tested &lt; 5)\nDo you feel the fantastic power of free pipelining with %&gt;% granted by excellent package magrittr? (Python packages really need to get their shit together, their names suck in comparison to R!) Can you understand absolute beauty that was bestowed upon us mere mortals, by Hadley Wickham the great creator of the dplyr and ggplot2 packages? Do you notice the absence of quotation marks around column names thanks to the holiness of lazy evaluation? The R people have been doing this routinely for more than 10 years now!\nDo you feel your absolute insignificance with your idiotic transformations riddled with the ubiquitous use of df = df[...]? Are you understanding why, in their eyes, you looked like a babbling, screaming monkey? Good! Time to blow your mind one last time. Have you heard of the method .pipe() in pandas? It’s limited, but it allows you to do some things like the pros! But do not abuse its power!\nBesides, look! Look how easy it is to wrap your head around a language you never learned when there is little to no syntactic crap.\nAlright, time to wrap up for now."
  },
  {
    "objectID": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#take-home-messages-for-dummies",
    "href": "posts/2025/2025_04_28_angry_pandas_guide/angry_pandas_tutorial.html#take-home-messages-for-dummies",
    "title": "The Angry Guide To Pandas Code (Dummy Edition, Part 1)",
    "section": "Take-Home Messages For Dummies",
    "text": "Take-Home Messages For Dummies\n\nNO FUCKING FOR LOOPS!\nNever use inplace=True. Only morons do so!\nDo not use backslashes \\ like a caveman. Just use a pair of brackets (), type the name of your dataframe, and press Enter.\n\n\nIf you do not follow these simple rules, I will find you, and I will unscrew your head. I will go out of my way to attack you. Even if your dead ass is sitting in the middle of the ocean, I will swim out into the middle of the ocean and friggin eat you! And then, I will bang your tuna boyfriend/girlfriend.\n\n\nInside of a chain, you can often reference your current dataframe inside a lambda expression.\n\nPerfect for creating new columns with assign().\nIt works really well with loc[] to filter rows on the fly.\nThere are other methods that accept this too.\n\nThere is a special place in hell for people who use methods that evaluate generic strings like .query(). A special place! Right next to child molesters and people who talk during movies!\nHow long can you even chain?\nThe R wizards tell us: “You can generally chain 5 to 10 transformation steps”. Beyond that, there is a higher risk for code to become less readable and more convoluted. Then you can just save the result in a nicely named variable describing exactly what your data is representing! Some person says you can do more without tampering with readability, I disagree, and then again, you are not some person…\n\nThat’s it for today. We will see more advance shit later. For now, I need to go choke on a bottle of Xanax!\n\nDisclaimer\nThis rant was largely inspired by an awesome guide for Metal Gear Rising: Revengeance by Vesperas. Its reading was a great source of banter and chuckles. My dear swearing fellow, you have my most sincere gratitude for helping a sucker like me progress at such a great game!"
  },
  {
    "objectID": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html",
    "href": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html",
    "title": "The Angry Guide To Pandas Code - Applying Vengeance (Part 2)",
    "section": "",
    "text": "Beware I have returned and I’m looking for blood!\nIt is time to revisit the pandas hell hole, and I’ll bring you all down there with me. You have no luck, I took methamphetamine instead of Xanax, so I’m up for intellectual murder in all kinds of degrees. In case you have the intellectual IQ of a burnt sushi, I will repeat once more: what we did last time was no optimization! We just did the bare minimum such that our code does not utterly suck, just enough for me to keep a human form and not bite your head off.\nLet’s recap for those who sniffed glue and ate the memo before reading it. There are clear signs for pandas code that should be drenched in gasoline and lit on fire:\nOn the opposite side, in the beautiful pandas world, you use method chaining to perform successive transformations. There, you can use a lambda to reference your current DataFrame in methods such as .assign() or .loc[].\nNow we’re going to see some more anti-patterns that will demonstrate that, apparently, a simple Google search is beyond your reach. After killing the King, today we’re killing the Queen and the whole court—i.e., the self-joins and .apply()."
  },
  {
    "objectID": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html#the-nasty-inbreeding-caused-by-self-joins",
    "href": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html#the-nasty-inbreeding-caused-by-self-joins",
    "title": "The Angry Guide To Pandas Code - Applying Vengeance (Part 2)",
    "section": "The Nasty Inbreeding Caused By Self-Joins",
    "text": "The Nasty Inbreeding Caused By Self-Joins\n\nThe Diagnostic\nYou need to recognize this horrendous habit of yours. It is a pandas anti-pattern, demonstrating clearly that you are just vomiting code. Here is how it will look in the wild:\nugly_as_fuck_df = ...\n\nstupid_calculation = (\n    ugly_as_fuck_df\n    .groupby([\"patiend_id\", \"year\"])\n    .agg({\"blood_pressure_value\": \"mean\"})\n    .rename(columns={\"blood_pressure_value\": \"avg_blood_pressure\"})\n    .reset_index()\n)\n\nugly_as_fuck_df = ugly_as_fuck_df.merge(\n    stupid_calculation,\n    on=[\"patient_id\", \"year\"],\n)\nDo you understand that your criminal and deviant mind has been feeding your poor DataFrame with its own offspring? Whenever you take an aggregated DF derived from a parent and merge it back with the parent, you are committing a cardinal sin. In reality, you’re just trying to create a new column with the result of an aggregation. There is a name for this: this is called a window function in good ol’ SQL, and in pandas it’s called a groupby().transform().\n\n\nThe Chained Solution\nThe .transform() method will produce an output with “the same indexes as the original object filled with the transformed values” meaning with the same DataFrame shape as before the .groupby().\nThis is how you would do it:\nmy_new_cool_df = (\n    ugly_as_fuck_df.assign(\n        avg_blood_pressure=lambda df: (\n            df.groupby([\"patient_id\", \"year\"])\n            [\"blood_pressure_value\"]\n            .transform(\"mean\")\n        )\n    )\n)\nFor the slow data scientists with limited cerebrospinal fluid circulation and still drinking the OOP/imperative Kool-Aid, the code above will produce a similar output to the following one (but without the intermediate results saved inside moronic variables):\nugly_as_fuck_df = ...\n\navg_blood_pressure = (\n    ugly_as_fuck_df.groupby([\"patient_id\", \"year\"])\n    [\"blood_pressure_value\"]\n    .transform(\"mean\")\n)\n\nugly_as_fuck_df[\"avg_blood_pressure\"] = avg_blood_pressure\nPathetic… You were close to greatness… My disappointment is immeasurable…\nDo you see how concise, clearer, and cooler the first version is? You are clearly expressing your motherfucking intent, which is:\n\ncreating (assigning) a new column\nderived from “blood_pressure_value” aggregated with the “mean” function\nthis calculation is done independently for each combination of “patiend_id” and “year”.\n\nDo you want more awesomeness in your awesomeness because I’m feeling generous? Your function inside the .transform will work if it returns a single value (like mean, min, max) OR if it returns a Series with the same size as the current group (like cumsum). In both cases, the .transform will be able to do the joining part for you and put the result of the calculation back with matching keys!\nYou can even supply your own aggregation functions as long as they return either a single value or an output with the same size (number of rows) as the input. But don’t start pulling some stupid-ass stunts like the ones we’re going to address next…"
  },
  {
    "objectID": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html#killing-all-the-apply",
    "href": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html#killing-all-the-apply",
    "title": "The Angry Guide To Pandas Code - Applying Vengeance (Part 2)",
    "section": "Killing All The Apply",
    "text": "Killing All The Apply\nNow that we killed the Queen, the self-joins, which serve no purpose at all except begging for more brioche like a product owner begs for more features, we have to take care of the court: the .apply().\nLike princesses and princes of the old days, most of them are lazy, parasitic, and notoriously harmful to the whole country, and thus should be eliminated without any shred of regret. Yet, in very few instances, some might hide a little bit of common sense, and one should just tolerate them, after stripping them of their privileges, of course! Fortunately, no need for a tribunal to tell them apart because one can recognize them at first glance.\n\nYou’re Too Dumb To Press “Dot” On Your Keyboard\nThis will be easy and quick; your shameful laziness will appear clear to all in a minute.\nHave you ever done something like this?\nmy_awesome_df[\"string_column\"].apply(lambda x: x.strip().split(\" \"))\nmy_awesome_df[\"datetime_column\"].apply(lambda x: x.date())\nHave you ever heard of vectorized operation? They are the ones that allow you to process gigabytes of data in a few seconds—yes, I said seconds, yes, with pandas! And do you know how ludicrous do you look? In order to use these vectorized operations, rather than rushing for the .apply, all you had to do was to write .str.something() or .dt.something_else() instead.\nmy_awesome_df[\"string_column\"].str.strip().str.split(\" \")\nmy_awesome_df[\"datetime_column\"].dt.date()\nDid your foggy brain, saturated with scrum vapors, happen to notice that the .str and .dt accessors made the stupid-ass names “string_column” and “datetime_column” completely irrelevant? Do you now understand how stupid you look with your automated use of .apply()? You exhibited a typical default flight response at the idea of looking at the documentation. You behaved like a screeching libertarian billionaire confronted with the idea that taxes are actually necessary to educate people and that he is going to have to pay. You were literally one keystroke away from producing something useful and still managed to fail…\nI am impressed by such dedication towards mediocrity. Anyway, go read the damn documentation about pandas datetimes and derived before we head to the next abomination.\n\n\nApply(…, index=1) Is For Zeroes\nDo you really want me to start digging a grave for your challenged neo-cortex that went on permanent vacation? This is a for loop in disguise, and you know what happens to people that use for loops around here, don’t you? Great, at least you seem to have developed enough survival instinct to calm my anger…for now!\nFrom now on, you will have only one God, and its glorious name is Vectorization! You will seek its favors at all times when you write pandas code. This will be your sole purpose in life, your only hope for salvation. You will follow the holy scriptures derived from the mathematical operators or the built-in .dt and .str accessors.\nYou may very sparsely consult a scripture from the nearby NumPy church when no solace can be found in the original divine texts. Beware: many heretics have abused the alternative scriptures and have fostered a blasphemous, unreadable hybrid, for which mercy compels us to shoot on sight. Especially, you will burn at the stake if I see any instance of nested np.where(), because we have seen the light of the glorious case_when in v2.2.0 (replacing the venerable np.select).\nOnce you do everything we mentioned so far, your code will be largely free from vomit projections or gangrenous infections. As long as you avoid those pitfalls carefully, pandas will become quite fast. To give you a ballpark idea, you should be able to process several GBs of real-life data in a matter of seconds—yes, I said seconds! If you reach such a state, an encounter with your code will not induce any ferocious fury fit that typically makes me reach for a flamethrower, but may actually leave me in a decent mood.\nThis leaves us in good dispositions to talk about some edge-cases.\n\n\nQuirks Of Apply On Multiple Columns\nNow it is time for a little confession; we’re approaching the only instance where I might, maybe, perhaps, acknowledge distantly that method chaining is slightly less readable than traditional OOP syntax. Here is a quick example:\nimport pandas as pd\nimport seaborn as sns\n\n\ndef z_score(col: pd.Series) -&gt; pd.Series:\n    return (col - col.mean()) / col.std()\nHere is the traditional way:\ndiamonds = sns.load_dataset(\"diamonds\")\ndiamonds[[\"carat\", \"depth\", \"table\", \"price\"]] = diamonds[\n    [\"carat\", \"depth\", \"table\", \"price\"]\n].apply(z_score)\ndiamonds.head()\nIndeed, applying the same transformation on multiple columns using pure method chaining requires a combination of lambdas, dict-comprehension, and unpacking… Now, this may look a little cabalistic for unaccustomed eyes:\ndiamonds = sns.load_dataset(\"diamonds\").assign(\n    **{\n        col: lambda df: z_score(df[col])\n        for col in [\"carat\", \"depth\", \"table\", \"price\"]\n    }\n)\ndiamonds.head()\nLet’s decompose this quickly:\n\n** to transform the dictionary into kwargs,\nthen the column name and a lambda to reference the current DataFrame,\nthe function to use inside the body of the lambda,\nthe columns to iterate over, which define our dict-comprehension,\nyou can even add a prefix/suffix by swapping the dictionary key with an f-string: f\"prefix_{col}\"/f\"{col}_suffix\".\n\nAgain, I will acknowledge that this may not be trivial to write, but at least I am sure you learned a thing or two.\nDon’t gloat, you overly inefficient entshitificator; this is not a moment of weakness from me. The atrocities you committed and indulged in disqualify you from making any desirable comment! The only reason for this is pandas’ venerable age and sadly the absence of a unified and clear syntax. But not all hope is lost, as larger animals will demonstrate much later.\n\nBut, Sir Rants, you just used for loops with a pandas DataFrame and you told me not to! That’s not fair!\n\nSigh… Do I really have to explain everything? Since your mind is equipped with the agility of a dead bird engulfed in an oil spill, I will articulate this more clearly. What you just witnessed in the former example is indeed a for loop. This for loop is used to create a finite collection of functions, but everything is done by the .assign() method. This for loop is not involved in any calculation. It does not iterate over the whole fucking DataFrame, nor perform 4 levels of nasty, nested group-by operations that even a moronic ape smashing on a keyboard would not dare to do!\n\n\nAdvanced Apply Operations\nHere we are mostly speaking about the pattern .groupby().apply(). Again there is some leeway, because on one hand it allows you to perform simply some operations that could be quite complex when written with a different syntax. On the other hand, it can become extremely fast a foot-gun loaded with unnecessary complexity, even faster than an executive sabotaging a good product because now it must use AI.\nRegardless, use a more straightforward and explicit syntax if possible:\n(\n    fancy_name_because_i_use_method_chaining\n    .groupby([\"column_name_1\", \"column_name_2\"])\n    .agg(\n        avg_price=(\"price\", \"mean\"),\n        avg_price_change=(\"price\", lambda x: x.diff().mean()),\n        awesome_feature=(\"scores\", function_defined_elsewhere),\n    )\n)\nThis very expressive syntax is usually a good seatbelt against our own stupidity while being only slightly more verbose. But since when is the number of characters you type the bottleneck of your work?"
  },
  {
    "objectID": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html#final-thoughts",
    "href": "posts/2025/2025_12_angry_pandas_guide_the_vengeance/angry_pandas_vengeance.html#final-thoughts",
    "title": "The Angry Guide To Pandas Code - Applying Vengeance (Part 2)",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nLet’s recap one more time how to write pandas that will not trigger the next maintainers of your code to spiral down into depression, hang themselves, and/or gouge their eyes out.\n\nNo …\n\nNo fucking for loops, Sir Rants, I learned the message!\n\nGood lad!\nNo inplace=True. It prevents method chaining and does nothing for memory.\nUse lambda df: df... to reference the current dataframe when filtering or creating new columns.\nDo not create DataFrame inbreeding with horrible self-joins. You should use the groupby().transform() which are similar to SQL window functions.\nThe use of .apply is typically done be weak minded people who have no clue of the absolute glory of Vectorization.\nThe only places where .apply can be tolerated are usually when using the same transformation on multiple columns for sake of readability, or after .groupby operations but beware of the complexity demon here!\nAny preprocessing of ≈10GBs on average hardware nowadays should definitely take less than a few 10s of seconds. Be extremely skeptic if you reach a minute or above.\n\nAgain, we are not doing any kind of optimization around here; we are just writing code that does not suck utterly and that is not a waste of CPU cycles. Method chaining is a good way to limit your own ability to do stupid shit, and it keeps you within a decent range of the “optimal” processing time you can hope to have in pure pandas.\nIf you use these simple rules of thumb, you will have a code that is not only surprisingly fast even for pandas but also extremely readable and debuggable. Your coworker will stop despising you when you hand them over some code, your manager will give you the employee of the month trophy, and your significant other won’t dump your sorry ass. (The last two promises are only binding for those who believe in cryptocurrency or the tooth-fairy)."
  },
  {
    "objectID": "twitter.html",
    "href": "twitter.html",
    "title": "Surprise…",
    "section": "",
    "text": "You really should get off that stupid platform that promotes the absence of thinking in 280 characters.\nThere is nothing of value that can be said in such a short format, its only a slogan.\nAnd don’t get me started on the nazi in chief who uses that for political interference and insider trading.\nSeriously, do a favor for your brain."
  }
]